# Legal Reach RAG - Conversion Plan for Your Codebase

## ğŸ¯ PROJECT STATUS
**Current Setup:**
- âœ… Virtual Environment: Active at `D:\ragbot\venv`
- âœ… Base Code: LocalAIAgentWithRAG (Ollama + Chroma + CSV)
- âœ… Hardware: RTX 3050 (4GB VRAM) + 16GB RAM
- âŒ Legal Documents: Not yet configured
- âŒ PDF/DOCX Processing: Not implemented

---

## ğŸ“Š ANALYSIS OF YOUR CURRENT CODE

### What You Have:
```
main.py          â†’ Uses Ollama LLM (llama3.2), ChatPromptTemplate
vector.py        â†’ Uses Chroma DB + OllamaEmbeddings + CSV data
requirements.txt â†’ Minimal deps (langchain, ollama, chroma, pandas)
```

### Key Architecture:
1. **Data Source**: CSV file (realistic_restaurant_reviews.csv)
2. **Vector DB**: Chroma (chrome_langchain_db)
3. **Embeddings**: OllamaEmbeddings (mxbai-embed-large)
4. **LLM**: OllamaLLM (llama3.2)
5. **Retrieval**: Top-5 similar reviews

### What Needs to Change:
| Component | Current | New |
|-----------|---------|-----|
| Data Input | CSV file | PDF + DOCX + TXT files |
| Processing | Pandas read_csv | pypdf + python-docx + text extraction |
| Chunking | None (whole reviews) | RecursiveCharacterTextSplitter (512 tokens) |
| Embeddings | OllamaEmbeddings | sentence-transformers/all-MiniLM-L6-v2 |
| Vector Store | Chroma DB | Chroma DB (same, optimized) |
| Prompt | Restaurant reviews | Legal document Q&A |
| Retrieval | k=5 | k=4 (legal context) |

---

## ğŸ› ï¸ CONVERSION STEPS

### STEP 1: Create Project Structure (5 mins)
```
D:\ragbot\venv\LocalAIAgentWithRAG-main\
â”œâ”€â”€ main.py (MODIFY)
â”œâ”€â”€ vector.py (MODIFY)
â”œâ”€â”€ requirements.txt (UPDATE)
â”œâ”€â”€ config.py (CREATE NEW)
â”œâ”€â”€ document_processor.py (CREATE NEW)
â”œâ”€â”€ utils.py (CREATE NEW)
â”œâ”€â”€ .env (CREATE NEW)
â”œâ”€â”€ test_setup.py (CREATE NEW)
â”œâ”€â”€ legal_documents/
â”‚   â”œâ”€â”€ pdfs/                    â† DROP YOUR LEGAL PDFs HERE
â”‚   â”œâ”€â”€ docx/                    â† DROP YOUR DOCX FILES HERE
â”‚   â””â”€â”€ txt/                     â† DROP TXT FILES HERE
â”œâ”€â”€ vectorstore/                 â† Auto-created (vector embeddings)
â””â”€â”€ logs/                        â† Auto-created (processing logs)
```

**Your Action:**
```powershell
cd D:\ragbot\venv\LocalAIAgentWithRAG-main
mkdir legal_documents\pdfs, legal_documents\docx, legal_documents\txt, vectorstore, logs
```

---

### STEP 2: Update requirements.txt (2 mins)

**Current:**
```
langchain
langchain-ollama
langchain-chroma
pandas
```

**New (adds PDF/DOCX support + GPU optimization):**
```
# Core LangChain
langchain==0.1.0
langchain-community==0.0.20
langchain-ollama

# Vector Store
langchain-chroma==0.4.22

# Document Processing
pypdf==4.0.1
python-docx==1.1.0

# Embeddings (LOCAL - no Ollama needed for embeddings)
sentence-transformers==2.3.1

# GPU Optimization for RTX 3050
torch==2.1.2
transformers==4.37.2
accelerate==0.26.1
bitsandbytes==0.42.0

# Utilities
pandas
python-dotenv==1.0.0
pynvml==11.5.0  # GPU monitoring
psutil==5.9.8   # System monitoring
```

**Why These Changes:**
- `pypdf` + `python-docx`: Load legal documents
- `sentence-transformers`: Fast embeddings (not Ollama - saves GPU)
- `bitsandbytes`: 8-bit quantization (save 50% memory)
- `pynvml`: Monitor GPU health

---

### STEP 3: Create config.py (for RTX 3050 optimization)

This file centralizes all settings so you can easily tune for your hardware.

---

### STEP 4: Create document_processor.py (core new file)

Handles: PDF â†’ DOCX â†’ TXT loading, chunking, embedding generation

---

### STEP 5: Update vector.py (modify existing)

Replace CSV loading with document loading, keep Chroma DB

---

### STEP 6: Update main.py (modify existing)

Replace restaurant prompt with legal template, add document upload UI

---

### STEP 7: Create utils.py (monitoring tools)

GPU monitoring, document caching, preprocessing

---

### STEP 8: Create .env (settings file)

API keys, model paths, GPU limits for RTX 3050

---

## ğŸ”„ WORKFLOW FOR YOUR CONVERSION

### Phase 1: Update Configuration (TODAY)
1. âœ… Create folder structure
2. âœ… Update requirements.txt
3. âœ… Install new packages
4. Create config.py
5. Create .env

### Phase 2: Add Document Processing (TODAY)
6. Create document_processor.py
7. Create utils.py
8. Update vector.py
9. Create test_setup.py
10. Test PDF/DOCX loading

### Phase 3: Update Main Application (TODAY)
11. Update main.py
12. Test full pipeline
13. Add GPU monitoring

### Phase 4: Test & Deploy (TODAY)
14. Add sample legal documents
15. Run end-to-end test
16. Verify GPU usage stays <85%

---

## âš ï¸ IMPORTANT MODEL SELECTION FOR RTX 3050

Your RTX 3050 has **4GB VRAM**. Here's what fits:

### âœ… RECOMMENDED SETUP:
| Component | Model | Size | GPU Memory |
|-----------|-------|------|------------|
| **Embeddings** | sentence-transformers/all-MiniLM-L6-v2 | 80MB | 0.2GB |
| **LLM** | OllamaLLM (llama3.2) | Runs on CPU/GPU | ~1-2GB if on GPU |
| **Vector DB** | Chroma | Varies | 0.5-1GB |
| **Total** | - | - | ~2-3GB safe |

### Why This Works:
- **Embeddings on GPU**: Fast (0.2GB) â† all-MiniLM-L6-v2
- **LLM on CPU or partial GPU**: llama3.2 is lightweight
- **Ollama Server**: Manages everything, prevents OOM
- **Leaves 1GB buffer**: For system stability

### âš ï¸ IF YOU GET GPU OUT OF MEMORY:
```python
# In config.py, change to:
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"  # Stays same (already minimal)
LLM_DEVICE = "cpu"  # Force LLM to CPU only, embeddings on GPU
CHUNK_SIZE = 256    # Reduce from 512
BATCH_SIZE = 2      # Reduce from 4
```

---

## ğŸ“‹ YOUR DOCUMENTS FOLDER STRUCTURE

Create this in `legal_documents/`:

```
legal_documents/
â”œâ”€â”€ pdfs/
â”‚   â”œâ”€â”€ contract_sample.pdf
â”‚   â”œâ”€â”€ terms_and_conditions.pdf
â”‚   â””â”€â”€ README.txt  â† "DROP PDFs HERE"
â”œâ”€â”€ docx/
â”‚   â”œâ”€â”€ legal_brief.docx
â”‚   â””â”€â”€ README.txt  â† "DROP DOCX FILES HERE"
â”œâ”€â”€ txt/
â”‚   â”œâ”€â”€ legal_text.txt
â”‚   â””â”€â”€ README.txt  â† "DROP TEXT FILES HERE"
â””â”€â”€ processed/      â† Auto-created by system
    â””â”€â”€ (internal use)
```

**To Add Documents:**
1. Copy your PDFs to `legal_documents/pdfs/`
2. Copy your DOCX to `legal_documents/docx/`
3. Copy TXT files to `legal_documents/txt/`
4. Run app: `streamlit run main.py` (if using Streamlit)
   OR: `python main.py` (if using CLI)
5. System auto-processes all documents

---

## ğŸ”§ HOW YOUR SYSTEM WILL WORK

### Current Flow (Restaurant Reviews):
```
CSV File (reviews.csv)
    â†“
vector.py: Load CSV with pandas
    â†“
Create embeddings (Ollama) 
    â†“
Store in Chroma DB
    â†“
main.py: User asks question
    â†“
Retrieve similar reviews (k=5)
    â†“
Ollama LLM generates answer
```

### New Flow (Legal Documents):
```
Legal Documents (PDFs/DOCX/TXT)
    â†“
document_processor.py: Extract text
    â†“
Split into chunks (512 tokens, 50 overlap)
    â†“
Create embeddings (sentence-transformers)
    â†“
Store in Chroma DB (same location, new collection)
    â†“
main.py: User asks question
    â†“
Retrieve relevant chunks (k=4)
    â†“
Ollama LLM generates legal answer
    â†“
Show sources + citations
```

**KEY DIFFERENCE:** Instead of whole CSV rows, you work with document chunks + metadata.

---

## ğŸ“ WHICH FILES TO KEEP / MODIFY / DELETE

### KEEP (Don't delete):
- âœ… `main.py` - Modify for legal prompt
- âœ… `vector.py` - Modify for document loading
- âœ… `requirements.txt` - Update versions
- âœ… `venv/` - Your virtual environment

### DELETE (Optional - but recommended):
- âŒ `realistic_restaurant_reviews.csv` - No longer needed
- âŒ `chrome_langchain_db/` - Old Chroma DB (new one created)

### CREATE (New files):
- âœ… `config.py` - Settings for RTX 3050
- âœ… `document_processor.py` - PDF/DOCX processing
- âœ… `utils.py` - Monitoring tools
- âœ… `.env` - Environment variables
- âœ… `test_setup.py` - Verify setup
- âœ… `.gitignore` - Hide large files

### FOLDERS (Create):
- âœ… `legal_documents/{pdfs, docx, txt, processed}`
- âœ… `vectorstore/` - New Chroma DB
- âœ… `logs/` - Processing logs

---

## ğŸš€ QUICK SETUP COMMANDS

```powershell
# 1. Navigate to your project
cd D:\ragbot\venv\LocalAIAgentWithRAG-main

# 2. Create folders
mkdir legal_documents\pdfs, legal_documents\docx, legal_documents\txt, vectorstore, logs

# 3. Backup old database (optional)
if (Test-Path chrome_langchain_db) { 
    Rename-Item chrome_langchain_db chrome_langchain_db_backup 
}

# 4. Create .gitignore
@"
__pycache__/
*.pyc
.env
vectorstore/
*.db
chrome_langchain_db*
legal_documents/pdfs/*
legal_documents/docx/*
legal_documents/txt/*
logs/
"@ | Out-File .gitignore

# 5. Update Python packages (in virtual env)
python -m pip install --upgrade pip
pip install -r requirements.txt
```

---

## âœ… NEXT: CREATE THE NEW FILES

Ready to proceed? I will create these files in order:

1. **config.py** - All settings for RTX 3050
2. **document_processor.py** - Load PDFs/DOCX, chunk text
3. **utils.py** - GPU monitoring, helpers
4. **vector.py** - Updated to load documents
5. **main.py** - Updated for legal RAG
6. **.env** - Environment settings
7. **test_setup.py** - Verify everything works

Each file will have:
- âœ… Detailed comments
- âœ… Type hints
- âœ… Error handling
- âœ… GPU optimization for RTX 3050
- âœ… Logging

---

## ğŸ“Š PROCESSING SPECS

### Document Processing:
- **Chunk Size**: 512 tokens (optimal for legal docs)
- **Chunk Overlap**: 50 tokens (maintains context)
- **Batch Size**: 4 embeddings at a time
- **GPU Memory**: ~3GB used, 1GB buffer

### Query Processing:
- **Retrieval**: Top 4 chunks (k=4)
- **LLM**: Ollama llama3.2 (on CPU or GPU as available)
- **Response Time**: 2-5 seconds per question
- **GPU Temp**: Keep <75Â°C (monitor via utils.py)

---

## ğŸ¯ YOUR GOALS

After conversion, you'll have:
âœ… Legal document RAG (PDF + DOCX + TXT support)
âœ… GPU-optimized for RTX 3050 (no crashes/overheating)
âœ… Same Chroma vector DB (familiar tech)
âœ… Same Ollama LLM (familiar setup)
âœ… Document source tracking + citations
âœ… Auto-processing for new documents
âœ… GPU health monitoring

---

## ğŸš¨ CRITICAL: BEFORE WE START

**Check Ollama is Running:**
```powershell
# Test Ollama service
curl http://localhost:11434/api/tags

# If error, start Ollama:
ollama serve
```

This should show available models. Your existing `llama3.2` should be there.

---

## â±ï¸ ESTIMATED TIME

- Setup folders + requirements: **5 mins**
- Create config.py + .env: **10 mins**
- Create document_processor.py: **15 mins**
- Create utils.py: **10 mins**
- Update vector.py: **10 mins**
- Update main.py: **15 mins**
- Test everything: **20 mins**
- **TOTAL: ~1.5 hours**

---

## âœ… Ready to Proceed?

Say "YES" and I will:
1. Create all 7 new files with GPU optimization
2. Adapt them to your existing code
3. Update your current files
4. Create test script
5. Give you step-by-step instructions

Your existing Ollama setup will continue to work - we're just replacing the data source from CSV â†’ Legal Documents.
